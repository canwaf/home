[
  {
    "objectID": "blog/posts/2025-02-23-dac-and-online-safety-act.html",
    "href": "blog/posts/2025-02-23-dac-and-online-safety-act.html",
    "title": "Data Are Cool: Disseminating My Online Safety Act Compliance",
    "section": "",
    "text": "The Online Safety Act is a piece of work, and not a good one either. Ofcom is not an excellent or communicative regulator. Because they are responsible for both setting the rules and components of enforcement they won’t provide advice which would prejudice their future enforcement. That said, there is a quick test to see whether a given website is in scope of the Online Safety Act:\nI’m going to cover these in turn, but the tl;dr is that I don’t believe Data Are Cool (DAC) is in scope of the Online Safety Act."
  },
  {
    "objectID": "blog/posts/2025-02-23-dac-and-online-safety-act.html#does-the-service-have-links-with-the-united-kingdom",
    "href": "blog/posts/2025-02-23-dac-and-online-safety-act.html#does-the-service-have-links-with-the-united-kingdom",
    "title": "Data Are Cool: Disseminating My Online Safety Act Compliance",
    "section": "Does the service have links with the United Kingdom?",
    "text": "Does the service have links with the United Kingdom?\nThere are two components to this test, first is whether UK users are a target market, and second is whether the service has a significant number of UK users. If you hit either one of them you’re in scope of the Online Safety Act.\n\nThe UK as a target market test\nFrom the first page of Ofcom’s Check if the regulations apply to your online service form has the following bullets which helps make a determination for whether the UK is a target market:\n\nYour online service is likely to have links with the United Kingdom if:\n\nIs designed for UK users;\nIs promoted or marketed toward UK users;\nGenerates revenue from UK users either:\n\ndirectly (e.g. via subscriptions or sales); or\nindirectly (e.g. through advertising to UK users, including people or organizations);\n\nIncludes functionalities or content that is tailored for UK users; or\nHas a UK domain or provides a UK contact address and/or telephone contact number.\n\n\nI don’t believe DAC is designed for UK users, it is not promoted or marketed, it generates no revenue, none of its content is tailored to UK users, and it doesn’t have a UK domain. It’s user base are my friends and family. There is no sign up capabilities.\n\n\nThe significant number of UK users test\nThe second component of this test is whether there are a significant number of UK users. Ofcom flat out refuse to define even by orders of mangitude what a significant number of UK users is.\nCandidly, DAC has 10 user accounts. I would reckon a significant number of UK users is well in the thousands, if not hundreds of thousands.\n\n\nUK links conclusion?\nIronically as I read this, DAC doesn’t demonstrate “links with the United Kingdom”. It neither has links to the United Kingdom, nor has a significant number of UK users. Using the Regulation Checker form, the answer immediately becomes “No, the Online Safety Act is not likely to apply to your online service.” This is a good start, but let’s check the remaining tabs anyways."
  },
  {
    "objectID": "blog/posts/2025-02-23-dac-and-online-safety-act.html#is-the-service-a-user-to-user-service",
    "href": "blog/posts/2025-02-23-dac-and-online-safety-act.html#is-the-service-a-user-to-user-service",
    "title": "Data Are Cool: Disseminating My Online Safety Act Compliance",
    "section": "Is the service a user-to-user-service?",
    "text": "Is the service a user-to-user-service?\nOfcom defines a user-to-user service as “an online service that allows its users to interact with each other.”\nDAC does this. It’s a social network. It’s a user-to-user service. Moving on."
  },
  {
    "objectID": "blog/posts/2025-02-23-dac-and-online-safety-act.html#do-you-provide-a-search-service",
    "href": "blog/posts/2025-02-23-dac-and-online-safety-act.html#do-you-provide-a-search-service",
    "title": "Data Are Cool: Disseminating My Online Safety Act Compliance",
    "section": "Do you provide a search service?",
    "text": "Do you provide a search service?\nOfcom defines a search service as “online service which is, or includes, a search engine. A search engine is a feature which enables users to search more than one website and/or database.”\nThe nature of Mastodon/Fediverse is that it is a search service. It’s a federated social network. It’s a search service; but also one that requires people to log in to search the Fediverse."
  },
  {
    "objectID": "blog/posts/2025-02-23-dac-and-online-safety-act.html#does-your-online-service-publish-or-display-pornographic-content",
    "href": "blog/posts/2025-02-23-dac-and-online-safety-act.html#does-your-online-service-publish-or-display-pornographic-content",
    "title": "Data Are Cool: Disseminating My Online Safety Act Compliance",
    "section": "Does your online service publish or display pornographic content?",
    "text": "Does your online service publish or display pornographic content?\nDAC does not have any alts (aka pornographic focused accounts) on the service; however some of DAC’s adult users have subscribed to the feeds of users elsewhere in the Fediverse who do post pornographic content. So we don’t publish it, but we do display it to logged-in users."
  },
  {
    "objectID": "blog/posts/2025-02-23-dac-and-online-safety-act.html#exemptions",
    "href": "blog/posts/2025-02-23-dac-and-online-safety-act.html#exemptions",
    "title": "Data Are Cool: Disseminating My Online Safety Act Compliance",
    "section": "Exemptions?",
    "text": "Exemptions?\nDAC isn’t exempt from the Online Safety Act in the form of their carve outs. Though there’s a small amount of snark from me because it exempts UK Parliament’s websites from the Online Safety Act. The UK Parliament’s petition website clearly would otherwise meet the threshold of a user-to-user service with UK targeting, and UK users in the millions. Sauce for the goose? Natch.\nAnywho…"
  },
  {
    "objectID": "blog/posts/2025-02-23-dac-and-online-safety-act.html#conclusion",
    "href": "blog/posts/2025-02-23-dac-and-online-safety-act.html#conclusion",
    "title": "Data Are Cool: Disseminating My Online Safety Act Compliance",
    "section": "Conclusion",
    "text": "Conclusion\nGoing back through these tests, I don’t believe DAC is in scope of the Online Safety Act mainly because I don’t believe it meets the thresholds established as “links to the United Kingdom”. It feels weird to phrase it this way, but…\n\nDAC isn’t designed for UK users (it isn’t designed beyond being a Mastodon instance);\nDAC isn’t promoted or marketed to UK users (it’s not promoted or marketed at all);\nDAC doesn’t generate revenue from UK users (I fund it out of my own pocket);\nDAC doesn’t have content tailored to UK users (it’s a social network for my friends and family);\nDAC doesn’t have a UK domain (it’s a vanity domain outside the country TLDs); and\nDAC doesn’t have a significant number of UK users (it has 10 users and I suspect significant is in the order of hundreds of thousands).\n\nIf Ofcom comes knocking, I’ll engage in good faith with them. Especially since I still plan on doing the Extra-Illegal Content/Harms risk assessments they require; however I won’t be killing my service because of the Online Safety Act.\nI’m not going to be complacent about this, but I’m not going to worry about it either.\nOfcom, if you’re reading this and want to get in touch, find all the details to contact me at Data Are Cool: about page."
  },
  {
    "objectID": "blog/posts/stryd-on-treadmill-interface/2023-02-05-stryd-on-a-treadmill.html",
    "href": "blog/posts/stryd-on-treadmill-interface/2023-02-05-stryd-on-a-treadmill.html",
    "title": "Stryd on a Treadmill: Interface Problems",
    "section": "",
    "text": "tl;dr the swipe based interactions on the Phone App for modifying incline on treadmill workouts is not safe and needs to be reimagined, ideally with a calculator button-style instant incline set option.\nBackground: A lot of my running at the moment is within the confines of a HIIT class (Barry’s Bootcamp), mainly to address my resistance training and improve core stability. Approximately half the class is on a treadmill regardless. The Treadmills are a Woodway make, and their user interface looks like the photo below. Speed on the right (miles per hour), incline on the left (percentage incline). There are also ways of modifying each single-press speed change by 0.1 units in two locations on the treads.\n\nJust to describe how to use the interface. You press 5 on the left side (Numbers are 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15), the treadmill blinks into action and begins increasing or decreasing the incline to the desired percentage incline. If you press 10 on the right side, the treadmill begins increasing or decreasing the speed to 10 miles per hour (Buttons are integers 0 to 12). A single button press is all that is required.\nHere’s what it looks like with the swipe interface on using the Stryd app on iOS.\nVideo link to video\nAt timestamp 1:40 my troubles really become apparent where I’m belting out the speed (sub 4 minute kilometres on an incline), but having to fight with the interface. Tapping the down button risks you swiping away the incline options, tapping anywhere risks just jiggling the interface and not registering the tap\nI know Stryd has structured incline runs for Treadmills. It’s a cool feature, and is something I will try out. But I have no idea what my incline asks are going to be because my trainers just shout a speed range and an incline at me. So I can’t use this function to set the incline by way of advancing laps.\nThe reason why I need track my incline this way is something you lovely folk know, Stryd can’t guess your incline on the treadmill and if I assume flat I leave a lot of effort unrecorded. My power curve has changed dramatically since I’ve started recording my inclines.\nWhat I want is a swipe-free interface something like this. I know it’s Frankenstein’s monster but I hope it gets the point across.\n\nAnother option would be 0-9 + an enter button. So you can go to 12% incline by pressing 1, 2, then enter.\nAnywho. That’s me for my very specific feature request in the guise of helping me safely run on a treadmill.\n(Cross posted from Stryd Club Forum, since I needed to host the video somewhere.)"
  },
  {
    "objectID": "blog/posts/2025-10-21-post-mortem.html",
    "href": "blog/posts/2025-10-21-post-mortem.html",
    "title": "POST Mortem: How Azure Application Gateways’s Missing 308 Killed Our Linked Data API",
    "section": "",
    "text": "In the Linked Data world, cool urls don’t change. That means in the RDF world you’re coining URIs that should be resolvable, and you pick the easiest one. Most people in the linked data world use http:// when coining URIs, even though today’s modern internet lives on https:// with upgrades handled by the service’s web stack.\nThe Water Quality service launching on the environment.data.gov.uk portal has a RESTful, Hydra API, and it supports a combination of GET and POST methods to retrieve data. The most useful endpoints living at /data are POST, as they can receive GeoJSON bounding boxes to query both geographic and observation data, though some uses don’t require a body.\nIn our testing we discovered that Python clients break when navigating the pagination of our service, but JavaScript works. WTF?"
  },
  {
    "objectID": "blog/posts/2025-10-21-post-mortem.html#the-http-redirect-status-code-landscape",
    "href": "blog/posts/2025-10-21-post-mortem.html#the-http-redirect-status-code-landscape",
    "title": "POST Mortem: How Azure Application Gateways’s Missing 308 Killed Our Linked Data API",
    "section": "The HTTP Redirect Status Code Landscape",
    "text": "The HTTP Redirect Status Code Landscape\nThe 300 series of HTTP Status Codes defined in RFC 7231, with 308 added in RFC 7538 help people navigate the internet automatically when resources move, protocols change, and they’re all quite useful.\n\n301 (Moved Permanently): The old guard—allows method changes\n302 (Found): Temporary and method-flexible\n303 (See Other): Forces GET (useful for POST-Redirect-GET pattern)\n307 (Temporary Redirect): Preserves method but temporary semantics\n308 (Permanent Redirect): The hero we need—permanent + method preservation\n\nThe issue It’s not a resource move (different URI), it’s a protocol upgrade (same resource, different scheme)."
  },
  {
    "objectID": "blog/posts/2025-10-21-post-mortem.html#link-data-apis-need-308",
    "href": "blog/posts/2025-10-21-post-mortem.html#link-data-apis-need-308",
    "title": "POST Mortem: How Azure Application Gateways’s Missing 308 Killed Our Linked Data API",
    "section": "Link Data APIs need 308",
    "text": "Link Data APIs need 308\nOur canonical URIs often use http:// scheme as protocol agnostic identifiers; however transport security requires HTTPS. Content negotiation and RDF payloads reference http:// URIs, and we don’t select the protocol on the fly in our responses. Both the link headers and the Hydra pagination links in our endpoint use the same URIs to help people navigate our pagination setup.\nSo you see how this is going to go? With POST getting redirected to GET will cause things to fall over when we erroneously get a 301 from Microsoft’s Application Gateway?"
  },
  {
    "objectID": "blog/posts/2025-10-21-post-mortem.html#the-azure-application-gateway-gap",
    "href": "blog/posts/2025-10-21-post-mortem.html#the-azure-application-gateway-gap",
    "title": "POST Mortem: How Azure Application Gateways’s Missing 308 Killed Our Linked Data API",
    "section": "The Azure Application Gateway Gap",
    "text": "The Azure Application Gateway Gap\nThe current available responses for a HTTP to HTTPS upgrade in Azure’s Application Gateway service are 301, 302, 304, and 307. It’s missing the semantically accurate and method-preserving 308. Not only that, we can’t target specific paths or entry points in the service. We are forced to chose between wrong semantics (i.e. temporary redirects) or broken clients (POST gets converted to GET)."
  },
  {
    "objectID": "blog/posts/2025-10-21-post-mortem.html#real-world-impact-client-behaviour-broken",
    "href": "blog/posts/2025-10-21-post-mortem.html#real-world-impact-client-behaviour-broken",
    "title": "POST Mortem: How Azure Application Gateways’s Missing 308 Killed Our Linked Data API",
    "section": "Real-World Impact: Client Behaviour Broken",
    "text": "Real-World Impact: Client Behaviour Broken\nLet’s be honest, the problem here is that Python is full of pedants (see: Pydantic), and the interpretation of RFC 7231 by the authors of its requests library have correctly implemented their redirect flag in the post() method. When a 301 redirect is encountered, requests converts the POST to a GET—which our /data endpoint doesn’t support, returning a 405 Method Not Allowed error.\nWhat should be a simple for loop navigating the link headers to collect a paginated dataset now requires custom redirect handling. What should be the simple contents of the while next_url: loop.\n# What breaks with 301:\nresponse = requests.post(next_url, headers=headers, data=\"\")\n# requests converts POST → GET on 301 redirect\n# Server responds: 405 Method Not Allowed\n# Pagination fails immediately\nBecomes the more convoluted:\n# Manual redirect handling to preserve POST method:\nresponse = requests.post(\n    next_url, \n    headers=headers, \n    auth=auth, \n    data=\"\", \n    allow_redirects=False  # Disable automatic redirect\n)\n\n# Handle redirect manually to keep POST\nif response.status_code in (301, 302, 307, 308):\n    next_url = response.headers['Location']\n    continue  # Re-POST to new URL\nNow I have to build my own redirect handling in Python because Microsoft has the semantics of their response codes wrong. I’m fine with it, but I want people to be able to be able to use our endpoint easily.\nOur front-end developers didn’t experience the same problem, which means that JavaScript’s fetch doesn’t do the same thing. This gives us an inconsistent API experience, and even with documentation being clear what’s going wrong with their code I’m still going to get support tickets that the thing is broken."
  },
  {
    "objectID": "blog/posts/2025-10-21-post-mortem.html#microsoft-fix-your-shit",
    "href": "blog/posts/2025-10-21-post-mortem.html#microsoft-fix-your-shit",
    "title": "POST Mortem: How Azure Application Gateways’s Missing 308 Killed Our Linked Data API",
    "section": "Microsoft: Fix Your Shit",
    "text": "Microsoft: Fix Your Shit\nYour Application Gateway redirect options aren’t complete. Give us a 308 code, allow us to be the pedants I want us to be. It would make a massive impact for the semantic web, improve our RESTful APIs, and follow modern HTTP patterns without breaking it for everyone else.\nStandards exist for a reason; it’s not a niche concern, as LLMs and Agentic AI usage becomes more and more common, having modern ways of accessing knowledge graphs and FAIR Data requires getting the semantics right everywhere — including in our HTTP response codes.\n@Azure: gimme the response code 308.\n\nNote: I have a support request asking for this behaviour. I expect Microsoft to change nothing."
  },
  {
    "objectID": "blog/posts/2025-02-20-jsonld-and-prefixes.html",
    "href": "blog/posts/2025-02-20-jsonld-and-prefixes.html",
    "title": "A small gotchya around jsonld 1.0, 1.1, and gen-delims from RFC3986",
    "section": "",
    "text": "I’m working on an exciting project which is the implementation of the SOSA standard, and as part of the project I wanted to use the envo ontology to provide context to the data contained therein.\nAs part of the development process I manually write RDF/Turtle to be sure I have the relationships correct for a single vertical component, and then I write a corresponding response in JSON-LD. From there I check for isomorphism of the two graphs to ensure that the JSON-LD is correct. The main reason why I manually serialize the JSON-LD is that the automatic conversion to JSON-LD is hideous; a well designed JSON-LD response could be indistinguishable from a resful JSON API response, but with the additional inclusions of various fields like @context and @id which can be ignored by users who would rather parse the data as a JSON object.\nNow that I’ve set up the scene, let’s get to the gotchya. I was writing a JSON-LD response for a vertical component, and I wanted to include the envo ontology as part of the context. I had the following JSON-LD:\n{\n  \"@context\": {\n    \"envo\": \"http://purl.obolibrary.org/obo/ENVO_\",\n    \"skos\": \"http://www.w3.org/2004/02/skos/core#\"\n  },\n  \"@id\": \"http://example.com/concept1\",\n  \"skos:prefLabel\": \"RIVER / RUNNING SURFACE WATER\",\n  \"skos:closeMatch\": \"envo:00000022\"\n}\nWhich unfortunately serilaized to\n&lt;http://example.com/concept1&gt; &lt;http://www.w3.org/2004/02/skos/core#closeMatch&gt; \"envo:00000022\" .\n&lt;http://example.com/concept1&gt; &lt;http://www.w3.org/2004/02/skos/core#prefLabel&gt; \"RIVER / RUNNING SURFACE WATER\" .\nAs you can see we don’t have the IRI of the envo object we expected (which would be http://purl.obolibrary.org/obo/ENVO_00000022).\nTurns out that JSON-LD 1.0 doesn’t support ending what in TTL is called @prefix when it doesn’t end in a gen-delim character as defined in RFC3986. tl;dr _s are out in prefixes with JSON-LD 1.0; however…\nJSON-LD 1.1 adopts the more permissive approach of IRI creation like RDF/TTL with a bit more in the context. By putting the prefix in an object’s @id, and setting a @prefix keyword to true, I finally got the result I wanted.\n{\n  \"@context\": {\n    \"@version\": 1.1,\n    \"envo\": {\n      \"@id\": \"http://purl.obolibrary.org/obo/ENVO_\",\n      \"@prefix\": true\n    },\n    \"skos\": \"http://www.w3.org/2004/02/skos/core#\"\n  },\n  \"@id\": \"http://example.com/concept1\",\n  \"skos:prefLabel\": \"RIVER / RUNNING SURFACE WATER\",\n  \"skos:closeMatch\": \"envo:00000022\"\n}\nI may be breaking some backwards compatibility here; for example neither the JSON-LD Playground nor the EasyRDF Converter serialize it differently to the first example in n3; however the likes of rdflib 7.1.3 does.\nFor me the JSON-LD “shape” is more important than giving supporting JSON-LD 1.0; and I can’t change the envo prefix. Necessity breeds breaking backwards compatibility."
  },
  {
    "objectID": "blog/posts/2025-06-15-running-effort-description.html",
    "href": "blog/posts/2025-06-15-running-effort-description.html",
    "title": "My own verison of Rating of Perceived Effort",
    "section": "",
    "text": "Today a friend I’m coaching made a comment about how a run of theirs “Felt good. Maybe a bit too fast for an easy run but still below tempo”. I had to remind him that an easy run is where you feel good about it.\nOffhadedly I invented a scale of how I want him to feel after various types of runs:\n\n\n\nRun type\nThe feeling I hope to inspire\n\n\n\n\nEasy\nFelt good, want to bring these vibes with me\n\n\nTempo\nAbsolutely knackered, I will collapse after I stretch\n\n\nEfforts\nI am in agony, and I want other people to feel this pain\n\n\nLong\nI want to end it all so this will stop\n\n\n\nYeah, it’s glib, but if you’re walking away from an effort session and your cardiovascular system isn’t crying out for revenge you’ve failed."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Andrew Fergusson",
    "section": "",
    "text": "Andrew Fergusson is a Linked Data Expert and a middling Python engineer. When he’s not banging on about metadata making everyone’s life easier, he’s probably banging the pavement in preparation for the next marathon."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Andrew Fergusson",
    "section": "Education",
    "text": "Education\nBSc (Hons) Computing & IT (Computing Science) with Statistics | Open University | October 2015 - September 2021"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About Andrew Fergusson",
    "section": "Experience",
    "text": "Experience\nLinked Data Expert Developer | Telespazio | September 2024 - Present\nLead Data Scientist | Office for National Statistics | May 2022 - September 2024\nSenior Data Engineer | Office for National Statistics | November 2020 - April 2022\nData Engineer | One Manchester | July 2019 - October 2020\nLead Developer (Data Engineering) | BNY Mellon | December 2013 - June 2019"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "andrew fergusson’s website",
    "section": "",
    "text": "Hi, I’m Andrew - a Linked Data Expert who’s spent the last decade turning messy data problems into elegant solutions across government and finance. I’m that rare breed who gets genuinely excited about metadata, believes good documentation is worth its weight in gold, and isn’t afraid to tell you exactly why your data pipeline needs both. When I’m not evangelizing about linked data or writing Python, you’ll find me at Barry’s Bootcamp, because apparently, I enjoy bashing keyboards, smashing the treadmill, and throwing weights around - much like I enjoy throwing around strong opinions about data engineering best practices.\nThere’s a lot to be said for a personal website without an all-consuming project. Over the last five years, I’ve built some cool things, but let me tell you about csvcubed - a tool born from the trenches of government data engineering that turned a painful linked data publishing process into something actually manageable. It took us from wrestling with presentational spreadsheets to pumping out 5-Star Linked Data at 10x the speed. If you’re into data engineering, linked data, or just enjoy stories about making complex things suck less, this one’s for you.\nNew projects I want to highlight and other bragging rites coming soon?\nFollow me on Mastodon to stay in the loop."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "POST Mortem: How Azure Application Gateways’s Missing 308 Killed Our Linked Data API\n\n\n\nprogramming\n\ntech\n\nw3c\n\nlinked-data\n\nrestful\n\nhttp\n\nwqa-api\n\nmicrosoft\n\nazure\n\napplication-gateway\n\n\n\n\n\n\n\n\n\nOct 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThe Art of Semantic Procrastination: Why I Use Blank Nodes for Concepts That Aren’t Mine\n\n\n\nprogramming\n\ntech\n\nw3c\n\nlinked-data\n\nrdf\n\njson-ld\n\nwqa-api\n\n\n\n\n\n\n\n\n\nAug 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPostgreSQL is the best triplestore\n\n\n\nprogramming\n\ntech\n\nw3c\n\nlinked-data\n\nrdf\n\njson-ld\n\n\n\n\n\n\n\n\n\nJun 26, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMy own verison of Rating of Perceived Effort\n\n\n\nrunning\n\ncoaching\n\nsports\n\nfitness\n\n\n\n\n\n\n\n\n\nJun 15, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nData Are Cool: Disseminating My Online Safety Act Compliance\n\n\n\ntech\n\npolitics\n\nfediverse\n\nself-hosting\n\nsocial-media\n\n\n\n\n\n\n\n\n\nFeb 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nA small gotchya around jsonld 1.0, 1.1, and gen-delims from RFC3986\n\n\n\nprogramming\n\ntech\n\nw3c\n\nlinked-data\n\nrdf\n\njson-ld\n\n\n\n\n\n\n\n\n\nFeb 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\ncsvcubed, a personal retrospective\n\n\n\nprogramming\n\ntech\n\nportfolio\n\n\n\n\n\n\n\n\n\nDec 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nStryd on a Treadmill: Interface Problems\n\n\n\nstryd\n\nrunning\n\nui\n\ncross-post\n\n\n\n\n\n\n\n\n\nFeb 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to roughdata!\n\n\n\nmeta\n\n\n\n\n\n\n\n\n\nJan 29, 2023\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/2025-06-26-postgres-is-the-best-triplestore.html",
    "href": "blog/posts/2025-06-26-postgres-is-the-best-triplestore.html",
    "title": "PostgreSQL is the best triplestore",
    "section": "",
    "text": "When folks want data they went it on a subject basis, making providing linked data that much easier. I have been exploring using Postgres with FastAPI and pydantic to serialize JSON-LD direct from SQL, to give users a familiar JSON RESTful API with content negotiated RDF baked in.\nCompared to the existing Jena-based API its throughput is two orders of magnitude faster, more reliable, and the data ingress doesn’t make me reconsider being in this domain.\nI hinted about this in February with a post about JSON-LD and prefixes. The API should be finished by the end of the month.\n(I’m on my second of hopefully two refactors.)"
  },
  {
    "objectID": "blog/posts/2025-08-30-cool-uris-dont-change.html",
    "href": "blog/posts/2025-08-30-cool-uris-dont-change.html",
    "title": "The Art of Semantic Procrastination: Why I Use Blank Nodes for Concepts That Aren’t Mine",
    "section": "",
    "text": "In the linked data world, there is always a temptation to boil the ocean. When building out a new API or even just a new dataset, there are so many concepts (SKOS:Concept and otherwise) that are undefined and uncoined which provide human context and you feel the pressure to define it in your RDF - at the risk of taking on too much and straying outside of your authority. I’ve faced that in the past while building out a linked data service at Office for National Statistics, and having been burnt by the numerous kettles we had going to define everything semantically. I’ve been determined to not make that mistake again.\nThe new API I’ve been developing for DEFRA is a Hydra/SOSA vocabulary based RESTful, content negotiated API for observational water quality data in England. The architecture of the service is FastAPI+PostGIS with a Next.JS frontend: the API doesn’t know anything about RDF; however it responds via JSON-LD by default with the JSON written in a way that people not familiar with RDF would appreciate.\nThe main payload of the API is sampling points (sosa:FeatureOfInterest) have samples & samplings (sosa:Sample, sosa:Sampling), which in turn have observations (sosa:Observation). Each of these levels have domain-specific types, classifications, and annotations which are necessary for the interpretation and discovery of these data; however no authoritative, public resource currently exists of these concepts.\nAs someone who lives FAIR, linked data, but knows most consumers of data neither understand nor care about it, what should I do? The answer isn’t to avoid these concepts - it’s to represent them responsibly until someone with actual authority shows up."
  },
  {
    "objectID": "blog/posts/2025-08-30-cool-uris-dont-change.html#procrastination-by-way-of-blank-nodes",
    "href": "blog/posts/2025-08-30-cool-uris-dont-change.html#procrastination-by-way-of-blank-nodes",
    "title": "The Art of Semantic Procrastination: Why I Use Blank Nodes for Concepts That Aren’t Mine",
    "section": "Procrastination by way of blank nodes",
    "text": "Procrastination by way of blank nodes\nMy solution is deterministic blank nodes. Instead of coining URIs for concepts I don’t own, I generate consistent blank nodes that can be reconciled later when authoritative sources emerge. This keeps my API stable while avoiding coining URIs I may eventually regret. Let me explain.\nPreviously I would have attempted to coin URIs for all my concepts, either at the dataset or higher level scope. For example, capturing the concept of running surface water from a river. In the source data for the API I have a table with a key and a label, the key acts as a notation.\n// You have no authority here, Jackie Weaver\n{\n  \"@id\": \"http://environment.data.gov.uk/id/sample-material/2AZZ\",\n  \"@type\": [\"skos:Concept\", \"sosa:FeatureOfInterest\"],\n  \"skos:prefLabel\": \"RIVER / RUNNING SURFACE WATER\",\n  \"skos:notation\": \"2AZZ\"\n}\nThe issue is I currently don’t have responsibility of the concept scheme for sample materials, and it’s also not online. I know all the values, and I have a copy of it to make the service work but it’s not within the scope of delivery for the water quality API. So instead of speaking with authority I’ve shifted to getting it down in code first to serve it via the API. How about as a blank node?\n// Procrastinating via blank nodes\n{\n  \"@id\": \"_:sampleMaterial-2AZZ\",\n  \"@type\": [\"skos:Concept\", \"sosa:FeatureOfInterest\"],\n  \"skos:prefLabel\": \"RIVER / RUNNING SURFACE WATER\",\n  \"skos:notation\": \"2AZZ\"\n}\nThe key here isn’t just using any blank node - it’s using a deterministic blank node identifier. By concatenating the concept scheme name with the notation (_:sampleMaterial-2AZZ), I ensure that every time this concept appears in my API responses, it gets the same blank node identifier.\n\nNote: This isn’t standard RDF blank node syntax - it’s my deterministic generation pattern from my source data. When serialized to actual RDF formats, these become proper blank nodes, but the consistent string ensures they all resolve to the same node across serializations. This isn’t just semantic pedantry - it has real practical benefits.\n\nWhen someone downloads multiple API responses and converts them to Turtle or N-Triples, all instances of _:sampleMaterial-2AZZ will be recognized as the same entity. Without this deterministic approach, you’d end up with multiple disconnected blank nodes for what should be the same concept, creating an unforgivable mess.\nHere’s what this looks like in practice - a real API response converted to Turtle:\ncurl -sSL --fail 'http://localhost:8000/sampling-point/53130070/sample?skip=0&limit=3&sampleMaterialType=2AZZ&complianceOnly=false' | rdfpipe -i json-ld -o ttl -\n@prefix dcterms: &lt;http://purl.org/dc/terms/&gt; .\n@prefix hydra: &lt;http://www.w3.org/ns/hydra/core#&gt; .\n@prefix skos: &lt;http://www.w3.org/2004/02/skos/core#&gt; .\n@prefix sosa1: &lt;http://www.w3.org/ns/sosa#&gt; .\n@prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; .\n\n&lt;http://localhost:8000/sampling-point/53130070/sampling/1506412&gt; a sosa1:Sampling ;\n    dcterms:type _:samplingPurpose-CA ;\n    sosa1:hasFeatureOfInterest &lt;http://localhost:8000/sampling-point/53130070&gt; ;\n    sosa1:hasResult &lt;http://localhost:8000/sampling-point/53130070/sample/1506412&gt; ;\n    sosa1:resultTime \"2001-08-08\"^^xsd:date ;\n    sosa1:startTime \"2000-08-18T12:20:00\"^^xsd:dateTime .\n\n&lt;http://localhost:8000/sampling-point/53130070/sampling/1510110&gt; a sosa1:Sampling ;\n    dcterms:type _:samplingPurpose-CA ;\n    sosa1:hasFeatureOfInterest &lt;http://localhost:8000/sampling-point/53130070&gt; ;\n    sosa1:hasResult &lt;http://localhost:8000/sampling-point/53130070/sample/1510110&gt; ;\n    sosa1:resultTime \"2000-10-05\"^^xsd:date ;\n    sosa1:startTime \"2000-09-20T12:00:00\"^^xsd:dateTime .\n\n&lt;http://localhost:8000/sampling-point/53130070/sampling/2303318&gt; a sosa1:Sampling ;\n    dcterms:type _:samplingPurpose-CA ;\n    sosa1:hasFeatureOfInterest &lt;http://localhost:8000/sampling-point/53130070&gt; ;\n    sosa1:hasResult &lt;http://localhost:8000/sampling-point/53130070/sample/2303318&gt; ;\n    sosa1:resultTime \"2001-06-07\"^^xsd:date ;\n    sosa1:startTime \"2000-11-29T00:01:00\"^^xsd:dateTime .\n\n&lt;http://localhost:8000/sampling-point/53130070/sample/1506412&gt; a sosa1:Sample ;\n    sosa1:isResultOf &lt;http://localhost:8000/sampling-point/53130070/sampling/1506412&gt; ;\n    sosa1:isSampleOf _:sampleMaterial-2AZZ,\n        &lt;http://localhost:8000/sampling-point/53130070&gt; .\n\n&lt;http://localhost:8000/sampling-point/53130070/sample/1510110&gt; a sosa1:Sample ;\n    sosa1:isResultOf &lt;http://localhost:8000/sampling-point/53130070/sampling/1510110&gt; ;\n    sosa1:isSampleOf _:sampleMaterial-2AZZ,\n        &lt;http://localhost:8000/sampling-point/53130070&gt; .\n\n&lt;http://localhost:8000/sampling-point/53130070/sample/2303318&gt; a sosa1:Sample ;\n    sosa1:isResultOf &lt;http://localhost:8000/sampling-point/53130070/sampling/2303318&gt; ;\n    sosa1:isSampleOf _:sampleMaterial-2AZZ,\n        &lt;http://localhost:8000/sampling-point/53130070&gt; .\n\n[] a hydra:Collection ;\n    hydra:member &lt;http://localhost:8000/sampling-point/53130070/sample/1506412&gt;,\n        &lt;http://localhost:8000/sampling-point/53130070/sample/1510110&gt;,\n        &lt;http://localhost:8000/sampling-point/53130070/sample/2303318&gt; ;\n    hydra:totalItems 129 ;\n    hydra:view [ hydra:first &lt;http://localhost:8000/sampling-point/53130070/sample?skip=0&limit=3&sampleMaterialType=2AZZ&complianceOnly=false&gt; ;\n            hydra:last &lt;http://localhost:8000/sampling-point/53130070/sample?skip=126&limit=3&sampleMaterialType=2AZZ&complianceOnly=false&gt; ;\n            hydra:next &lt;http://localhost:8000/sampling-point/53130070/sample?skip=3&limit=3&sampleMaterialType=2AZZ&complianceOnly=false&gt; ] .\n\n_:sampleMaterial-2AZZ a skos:Concept,\n        sosa1:FeatureOfInterest ;\n    skos:notation \"2AZZ\" ;\n    skos:prefLabel \"RIVER / RUNNING SURFACE WATER\" .\n\n_:samplingPurpose-CA a skos:Concept ;\n    skos:notation \"CA\" ;\n    skos:prefLabel \"COMPLIANCE AUDIT (PERMIT)\" .\nNotice how _:sampleMaterial-2AZZ appears once in the graph but is referenced by multiple samples - exactly what we want."
  },
  {
    "objectID": "blog/posts/2025-08-30-cool-uris-dont-change.html#when-the-kettles-come-out-reconciliation-without-regret",
    "href": "blog/posts/2025-08-30-cool-uris-dont-change.html#when-the-kettles-come-out-reconciliation-without-regret",
    "title": "The Art of Semantic Procrastination: Why I Use Blank Nodes for Concepts That Aren’t Mine",
    "section": "When the kettles come out: reconciliation without regret",
    "text": "When the kettles come out: reconciliation without regret\nThe beauty of this approach is that when the authoritative concept scheme eventually goes online (and it will, because I’m also building that service), I can simply add reconciliation triples without breaking anything. This is where semantic versioning becomes your friend - adding triples is a patch-level change at most. It neither changes the shape of the API’s JSON, nor previously coined URIs.\n// Future state - same identifier, now with authority\n{\n  \"@id\": \"_:sampleMaterial-2AZZ\",\n  \"@type\": [\"skos:Concept\", \"sosa:FeatureOfInterest\"],\n  \"skos:prefLabel\": \"RIVER / RUNNING SURFACE WATER\",\n  \"skos:notation\": \"2AZZ\",\n  \"skos:exactMatch\": \"http://environment.data.gov.uk/def/sample-material/2AZZ\",\n  \"rdfs:definedBy\": \"http://environment.data.gov.uk/def/sample-material/\"\n}\nNow I can fire up those kettles I avoided earlier. The blank node stays the same, existing API consumers continue to work, but new consumers can follow the skos:exactMatch to the authoritative source. Cool URIs don’t change, and neither will these deterministic blank nodes.\nThis approach scales beautifully across different concept schemes. Whether it’s determinands that eventually align with QUDT vocabularies, geographic regions that get proper Ordnance Survey URIs, or measurement units that find their way into authoritative registries - the pattern remains the same. Add the reconciliation triples when you have them, leave the blank nodes as stable anchors within the service.\n// And it even supports multiple reconciliation targets\n{\n  \"@id\": \"_:sampleMaterial-2AZZ\",\n  \"@type\": [\"skos:Concept\", \"sosa:FeatureOfInterest\"],\n  \"skos:prefLabel\": \"RIVER / RUNNING SURFACE WATER\",\n  \"skos:notation\": \"2AZZ\",\n  \"skos:exactMatch\": \"http://environment.data.gov.uk/def/sample-material/2AZZ\",\n  \"rdfs:definedBy\": \"http://environment.data.gov.uk/def/sample-material/\",\n  \"skos:closeMatch\": \"http://purl.obolibrary.org/obo/ENVO_00000022\"\n}\nIn a perfect world, every concept would have an authoritative URI from day one. In the real world, sometimes the most responsible thing you can do is admit you’re not the authority - yet. Deterministic blank nodes let you build useful services today while keeping the door open for proper reconciliation tomorrow. It’s procrastination with a purpose."
  },
  {
    "objectID": "blog/posts/2024-12-15-csvcubed.html",
    "href": "blog/posts/2024-12-15-csvcubed.html",
    "title": "csvcubed, a personal retrospective",
    "section": "",
    "text": "csvcubed is a tool for building CSV-W files. If you’re wondering what the hell CSV-W is, it’s basically CSV files with extra metadata that provides context and makes them play nice with linked data. It was born out of necessity when I was working on the ONS’s Integrated Data Service’s Dissemination service. Our end product was 5-Star Linked Data, and we needed a way to convert CSV files into RDF. I joined the project during the tail end of 2020 during lockdown as a data engineer and the pipeline for creating CSV-W was a bit of a mess but born of necessity.\nMy onboarding at ONS was great - I was quickly indoctrinated into the power of linked data and the associated standards. My actual job though? Unfucking presentational spreadsheets that locked away most of ONS’s statistical publications. Who wants to unpivot data just to do analysis? Not me, and honestly not the analysts producing them either - nobody wants to do analysis on pivoted data.\nThe tool I initially learned for generating CSV-W was databaker, which Sensible Code knocked together during a hackathon. It did the job of creating tidy data, but that was about it. Our pipeline was ultimately this Airflow-orchestrated mess: scrape a publication’s latest spreadsheet, use databaker to unpivot it, describe the data using something called gss-utils (to which I will not link because CVEs and archived repo related reasons), build a CSV-W, use Swirrl’s csv2rdf tool to convert the CSV-W to RDF, and then publish the RDF to the ONS’s linked data platform (now defunct but it was called IDS Data Explorer). This was a lot of steps, and the pipeline was brittle. Kicking Airflow was a regular occurance.\nI’m a bit of a diva, and sometimes divas are good for getting shit done. The first thing I started to change was the unpivoting process. The databaker tool needed to go - it was slow, unpythonic, and didn’t provide any transferrable skills. Deadend tools are a horrible career investment, so I switched to pandas and dragged the other data engineers with me. This was a good first step, but the reproducibility was still a mess. It was time to build a tool that standardized the production of CSV-W files.\ngssutils was probably my biggest bugbear - while it technically did the job of producing CSV-W files, it was about as transparent as a brick wall. Extending it was a pain in the ass, and adding new predicates to our data was even worse. Since our target was RDF Cube Vocabulary, I conspired with a good work-friend (who went by robons on github) to build a tool that would actually make sense of this CSV-W building process. We originally called it csvwlib but ultimately named it csvcubed.\nHere’s the thing about generating linked observational data - it’s a massive problem space. The RDF Cube Vocabulary is a solid standard, but when you throw in the requirement for harmonization before publication, it’s daunting. RDF Cubes split tabular data into three parts: dimensions (what you slice and dice by), attributes (context for your observations), and the actual observations themselves. In our idealistic world, each dimension needed a code list (basically a SKOS concept scheme), and ideally, you’d just reuse one that already existed in our service. This meant that in the old way of building a cube, you either had to reconcile definitions between datasets to reuse them, or manually write a new concept scheme as a CSV-W. Fun times.\nTo write a RDF Cube-bound CSV-W, you had to write at least one other CSV-W, or worse, reconcile concept definitions across multiple datasets. This was a massive headache for my fellow data engineers - we weren’t statistical subject matter experts, we were data engineers who just wanted to build pipelines that actually worked and could scale. That’s where csvcubed came in.\nThe idea behind csvcubed was simple: you give it a tidy data CSV, and it figures out the rest. Using keywords in the column headers, it works out the dimensions, attributes, and observations of the cube. It automatically creates code lists and concept schemes for dimensions. Suddenly, building a cube wasn’t such a pain in the ass, and the pipeline actually made sense. The tool was a hit - we went from pushing out 1 publication per data engineer per week to smashing out 10 publications per data engineer per week at our peak.\nI’ve moved on since then - these days I’m virtualizing RDF data using ontop in my new gig providing linked data services for DEFRA. But I hope csvcubed keeps being useful for people in the linked data world. I’ve used it a few times in my new role, so I’m still eating my own dog food.\nI’m now not only a diva but fully a linked data partisan. ONS turned me into a true believer, and I’m not looking back. You can claim to do linked data with a black box tool, but let’s be real - if you can’t see how it works, you can’t claim it’s FAIR or 5-Star Linked Data."
  },
  {
    "objectID": "blog/posts/2023-01-29-welcome-to-rougdata.html",
    "href": "blog/posts/2023-01-29-welcome-to-rougdata.html",
    "title": "Welcome to roughdata!",
    "section": "",
    "text": "Turns out, I do need a blog, and it also turns out having backups of my blog posts is a good idea. So here we are.\nThat said, this particular recreation of this blog will be done using the Wayback machine because I actually can’t find my old svbtle hosted blog backups. Glad I was worthy for the archiving, Internet Archive\nI’ll post the old stuff as I determine its suitability for the Internet."
  }
]